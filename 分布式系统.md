# Introduction

* MIT 6.824
	* parallelism
	* fault tolerance
	* physical
	* security / isolated
* Challenges
	* concurrency
	* partial failure
	* performance
* Infrastructure - Abstraction
	* Storage
	* Communication
	* Computation
* TOPIC
	* Performance
		* 2 times computers -> 2 times throughput 
	* Fault Tolerance
		* Availability
		* Recoverability
		* N-V storage
		* Replication
	* Consistency
		* example : K-V System

# RPC and Threads

* Reason use multi threads
	* I/O concurrency
	* Parallelism
	* Convenient
* Thread challenges
* Coordination
	* channels
	* sync.Cond
	* Wait Group
* Deadlock
* RPC semantic under failures
	* at least once
	* at most once
	* exactly once

# GFS

* CAP

	* performance  -> sharding (分片) 

	* faults  -> tolerance 

	* tolerance -> replication 

	* peplication -> inconsisency

	* consistency -> low performance
* GFS
	* Big & Fast
	* Global
	* Sharding
	* Automatic recovery
	* Single data center
	* Internal use
	* Big sequential access
* MASTER DATA
  * filename  -> array of chunks (nv)
  * handle -> list of chunk servers; version #(nv); primary; lease expiration
  * LOG, CHECKPOINT  in DISK

# VMware-FT	

* Primary-Backup Replication

* Fault tolerance

	* Failures
		* √ fail-stop faults
		* × Bugs

* 两种同步方式

	* State Transfer
		* memory

	* Replicated State Machine
		* op
			* PROBLEMS
				* what state? 复制什么state （paper： all memory & register)
				* Primary/Backup sync
				* Cut-over
				* Anomalies
				* New replicas

* Non-det operations
	* Inputs - packet - data + interrupt
	* weird instructions
	* multi core 
		* 例如不同线程以不同的顺序获得锁
		* paper中是单核的 所以这个paper并不实用
		* 但硬件是在单核VMM之下的 所以是可以在多核硬件下用
	* Log entry (主备之间通过log channel 交互)
		* instruction #
		* type
		* data

* Output Rule
	* 等待backup ACK
	* 发送output
* Duplicated Output
	* 依赖TCP解决
* Test And Set
	* 第三方仲裁
	* 避免脑裂 俩都想成为primary

# GO Thread & Raft

* 为什么使用多线程
	* 多核
	* 表达清晰（定时任务）
	* 阻塞操作
* 多个线程操作共享变量时最好都持有一把锁，因为你想不到编译器会进行怎样的优化

# Raft

* Brain Split
	* 策略1： Client 收不到全部确认就不用  那还不如单体
	* 策略2： Client 收到一个就凑合着用 可能会出现脑裂

* Majority vote
	* 2 Of 3 超过所有半数支持 避免了脑裂
	* 2 f + 1  Of  f fault   有2f + 1台服务器 就能够承受f台故障
	
* 

  * 服务端处理put请求

    * Leader 接受到请求 从应用层将put发给Raft层
    * Raft层会把put操作加入的日志中  会和 所有 followers 通信，直到有半数以上的副本把这个put操作添加到日志中
    * Raft告诉应用层这个操作的日志已经复制好了，应用层可以执行这个操作了
    * 应用层操作完成后返回结果给客户端
    * ![image-20220914144515216](/Users/pangg/Documents/GitHub/xuexi/images/分布式系统/image-20220914144515216.png)时序图 AE = Append Entry(log)   EX = Execute

  * Leader选举

    * 最重要的特性 一个term 只有 一个 leader
    * 选举流程
      * 每个replicas 有一个Election Timer 他认为时间到了没有收到leader的包就开始重新选举 每次收到appendentiries 就重新计时
      * 这个replica 将term++ (毕竟一个Term 只能有一个leader) 然后发起一次选举Request Vote
      * 不是说leader挂了才会选举，如果出现了网络分区，leader可能在少数派里面浪，他还是以为自己是leader（这个leader的append不会被采用 因为没有获得多数派认可），但其实另一个网络分区的 大多数已经选举出新的leader来了
      * 选举获胜之后的新leader立即向所有服务器发送AppendEntries 不需要说我是leader  因为只有leader被允许发送appendentries

  * 日志同步

    * 新leader发送的第一条appendentry 包括上一条日志的序号和term 用于follower同步
    * 如果follower发现上一条日志序号或者term不一致 返回false
    * leader会维护所有follower的最后一个日志序号 前面一个日志序号不一致的话 将其减1 发送最后两个日志给follower同步 还会包括倒数倒数第三个的日志序号和term号 不一致的话follow返回false 再重新发以此类推
    * ![image-20220914161437462](/Users/pangg/Documents/GitHub/xuexi/images/分布式系统/image-20220914161437462.png)

  * 谁来成为Leader?

    * 为什么不适用最长log优先？考虑如下场景，两个8都已经被服务器响应给客户端了，这时候s1再成为leader就会强行把两个8顶掉

      ![image-20220914184238248](/Users/pangg/Documents/GitHub/xuexi/images/分布式系统/image-20220914184238248.png)

    *  election restriction：

      vote yes only if higher term  in last entry, or same last term, ≥ log len，最后一个term 大于等于我 且长度大于等于我 我才同意（实际上就是选最后term最大的，最大的里面选log最长的）

    * FAST BACKUP

      Follower 返回足够多的信息，让leader以term为单位回退，而不是以log为单位
      
    * Persistence
    
      * Log 断电后重建应用程序状态
      * currentTerm 和 VoteFor 保证一个term只有一个Leader，否则断电前投一票断电后还会投一票
      * 磁盘太慢 ： 写机械磁盘 数量级是 10ms SSD 数量级 0.1ms write() fsync()  批量请求
    
    * Snapshot
    
      * 日志存太多的话压缩成快照
      * 就会和应用程序强绑定
      * leader和follower的快照同步又成了问题
    
    * Linearizability
    
      * 定义一个多副本服务什么是正确：表现得就像一个不会发生故障的服务器
    
      * 两条规则，如果操作顺序出现了循环，就说明不满足线性一致性
    
        1. 如果一个操作在另一个操作开始前就结束了，那么这个操作必须在执行历史中出现在另一个操作前面。
        2. 执行历史中，读操作，必须在相应的key的写操作之后。
    
        换句话说
    
        1. 请求顺序与实际时间匹配
        2. 每个读看到的都是前一个写的值
    
        如果能构造出这样的序列，说明这个请求历史记录是线性一致性的。
    
        线性一致性是针对客户端看到的历史记录的，我们只能通过观察记录来判断系统是否满足线性一致性

# ZooKeeper

* what is zk
  * 你可以设计应用程序与raft进行交互，但是raft不是独立的、通用的，而是需要单独设计的
  * 提供独立的 通用的 容错的 方式构建多副本分布式系统
  * zab层（代替raft，基本上等于raft）来通过选举、appendEntry维护replicas一致性
  * replicas导致 所有请求负载到leader上，leader成为主要性能瓶颈 因此通过读写分离提高性能
  * 无法确保follower读到up to date的数据，不能保证的话系统就无法提供线性一致性，实际上zk没有提供线性一致性读，它有自己的一致性定义
  * 不提供线性一致性后 系统是否还可用是我们要考虑的一个问题
* zk一致性保证
  * 写请求线性一致性
  * 任何一个客户端的请求都会按照客户端指定顺序执行，FIFO客户端序列
    * 客户端第一个读请求在某个log位置执行，第二个读请求应该不晚于这个位置，第二个读请求至少看到第一个读请求之前的状态
    * 通过服务端返回zxid 服务端记录，下一次读必须要求≥这个zxid
    * 如果发送了写请求+读请求，写请求没完成之前，读请求会暂缓，以确保FIFO
    * 一个人的读写请求是FIFO的，但是如果A写，B读同时发生，不保证B能看到A的写 zk单个客户端的请求是线性一致性的
* 怎么保证读到最新数据
  * Sync 请求相当于一个写请求，让副本进行同步
* ZooKeeper API
  * 通过版本号实现mini transection, test and set机制		
  * 通过文件创建获取锁，删除文件释放锁，和watch等待锁释放

