# Introduction

* MIT 6.824
	* parallelism
	* fault tolerance
	* physical
	* security / isolated
* Challenges
	* concurrency
	* partial failure
	* performance
* Infrastructure - Abstraction
	* Storage
	* Communication
	* Computation
* TOPIC
	* Performance
		* 2 times computers -> 2 times throughput 
	* Fault Tolerance
		* Availability
		* Recoverability
		* N-V storage
		* Replication
	* Consistency
		* example : K-V System

# RPC and Threads

* Reason use multi threads
	* I/O concurrency
	* Parallelism
	* Convenient
* Thread challenges
* Coordination
	* channels
	* sync.Cond
	* Wait Group
* Deadlock
* RPC semantic under failures
	* at least once
	* at most once
	* exactly once

# GFS

* CAP

	* performance  -> sharding (分片) 

	* faults  -> tolerance 

	* tolerance -> replication 

	* peplication -> inconsisency

	* consistency -> low performance
* GFS
	* Big & Fast
	* Global
	* Sharding
	* Automatic recovery
	* Single data center
	* Internal use
	* Big sequential access
* MASTER DATA
  * filename  -> array of chunks (nv)
  * handle -> list of chunk servers; version #(nv); primary; lease expiration
  * LOG, CHECKPOINT  in DISK

# VMware-FT	

* Primary-Backup Replication

* Fault tolerance

	* Failures
		* √ fail-stop faults
		* × Bugs

* 两种同步方式

	* State Transfer
		* memory

	* Replicated State Machine
		* op
			* PROBLEMS
				* what state? 复制什么state （paper： all memory & register)
				* Primary/Backup sync
				* Cut-over
				* Anomalies
				* New replicas

* Non-det operations
	* Inputs - packet - data + interrupt
	* weird instructions
	* multi core 
		* 例如不同线程以不同的顺序获得锁
		* paper中是单核的 所以这个paper并不实用
		* 但硬件是在单核VMM之下的 所以是可以在多核硬件下用
	* Log entry (主备之间通过log channel 交互)
		* instruction #
		* type
		* data

* Output Rule
	* 等待backup ACK
	* 发送output
* Duplicated Output
	* 依赖TCP解决
* Test And Set
	* 第三方仲裁
	* 避免脑裂 俩都想成为primary

# GO Thread & Raft

* 为什么使用多线程
	* 多核
	* 表达清晰（定时任务）
	* 阻塞操作
* 多个线程操作共享变量时最好都持有一把锁，因为你想不到编译器会进行怎样的优化

# Raft

* Brain Split
	* 策略1： Client 收不到全部确认就不用  那还不如单体
	* 策略2： Client 收到一个就凑合着用 可能会出现脑裂

* Majority vote
	* 2 Of 3 超过所有半数支持 避免了脑裂
	* 2 f + 1  Of  f fault   有2f + 1台服务器 就能够承受f台故障
	
* 

  * 服务端处理put请求

    * Leader 接受到请求 从应用层将put发给Raft层
    * Raft层会把put操作加入的日志中  会和 所有 followers 通信，直到有半数以上的副本把这个put操作添加到日志中
    * Raft告诉应用层这个操作的日志已经复制好了，应用层可以执行这个操作了
    * 应用层操作完成后返回结果给客户端
    * ![image-20220914144515216](/Users/pangg/Documents/GitHub/xuexi/images/分布式系统/image-20220914144515216.png)时序图 AE = Append Entry(log)   EX = Execute

  * Leader选举

    * 最重要的特性 一个term 只有 一个 leader
    * 选举流程
      * 每个replicas 有一个Election Timer 他认为时间到了没有收到leader的包就开始重新选举 每次收到appendentiries 就重新计时
      * 这个replica 将term++ (毕竟一个Term 只能有一个leader) 然后发起一次选举Request Vote
      * 不是说leader挂了才会选举，如果出现了网络分区，leader可能在少数派里面浪，他还是以为自己是leader（这个leader的append不会被采用 因为没有获得多数派认可），但其实另一个网络分区的 大多数已经选举出新的leader来了
      * 选举获胜之后的新leader立即向所有服务器发送AppendEntries 不需要说我是leader  因为只有leader被允许发送appendentries

  * 日志同步

    * 新leader发送的第一条appendentry 包括上一条日志的序号和term 用于follower同步
    * 如果follower发现上一条日志序号或者term不一致 返回false
    * leader会维护所有follower的最后一个日志序号 前面一个日志序号不一致的话 将其减1 发送最后两个日志给follower同步 还会包括倒数倒数第三个的日志序号和term号 不一致的话follow返回false 再重新发以此类推
    * ![image-20220914161437462](/Users/pangg/Documents/GitHub/xuexi/images/分布式系统/image-20220914161437462.png)

  * 谁来成为Leader?

    * 为什么不适用最长log优先？考虑如下场景，两个8都已经被服务器响应给客户端了，这时候s1再成为leader就会强行把两个8顶掉

      ![image-20220914184238248](/Users/pangg/Documents/GitHub/xuexi/images/分布式系统/image-20220914184238248.png)

    *  election restriction：

      * vote yes only if higher term  in last entry, or same last term, ≥ log len，最后一个term 大于等于我 且长度大于等于我 我才同意（实际上就是选最后term最大的，最大的里面选log最长的）

    * FAST BACKUP

      * 